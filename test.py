import torch  # å¼•å…¥ PyTorch æ·±åº¦å­¸ç¿’æ¡†æ¶
from dataset import get_data_transforms  # å¾ dataset.py åŒ¯å…¥è³‡æ–™å¢å¼·æ–¹æ³•èˆ‡è³‡æ–™è¼‰å…¥å‡½å¼
# from torchvision.datasets import ImageFolder  # åŒ¯å…¥ PyTorch å®˜æ–¹åœ–ç‰‡è³‡æ–™é›†è®€å–å·¥å…·
import numpy as np  # åŒ¯å…¥æ•¸å€¼è¨ˆç®—å‡½å¼åº« NumPy
# from torch.utils.data import DataLoader  # PyTorch çš„è³‡æ–™è¼‰å…¥å™¨ (batch/è¿­ä»£å™¨)
from resnet import  wide_resnet50_2  # åŒ¯å…¥è‡ªå®šç¾©çš„ ResNet æ¨¡å‹
from de_resnet import  de_wide_resnet50_2  # åŒ¯å…¥ ResNet çš„åå·ç©è§£ç¢¼å™¨
from dataset import MVTecDataset  # åŒ¯å…¥ MVTec è³‡æ–™é›†å®šç¾©é¡åˆ¥ (ç‘•ç–µæª¢æ¸¬ç”¨)
from torch.nn import functional as F  # åŒ¯å…¥ PyTorch å¸¸ç”¨å‡½æ•¸ (å¦‚æ¿€æ´»ã€å·ç©ã€æ’å€¼ç­‰)
from sklearn.metrics import roc_auc_score  # åŒ¯å…¥ sklearn çš„ ROC AUC è©•ä¼°æŒ‡æ¨™
import cv2  # OpenCVï¼Œç”¨æ–¼å½±åƒè™•ç†
import matplotlib.pyplot as plt  # ç¹ªåœ–å·¥å…· Matplotlib
from sklearn.metrics import auc  # AUC è¨ˆç®—å‡½å¼
from skimage import measure  # å½±åƒè™•ç†å·¥å…· (å€åŸŸæ¨™è¨˜ã€å€åŸŸå±¬æ€§)
import pandas as pd  # è³‡æ–™åˆ†æå·¥å…· Pandas
from numpy import ndarray  # åŒ¯å…¥ ndarray å‹åˆ¥åˆ¥å
from statistics import mean  # Python å…§å»ºå¹³å‡æ•¸è¨ˆç®—
from scipy.ndimage import gaussian_filter  # é«˜æ–¯æ¿¾æ³¢å™¨
from sklearn import manifold  # æ›¼å“ˆé “/æµå½¢å­¸ç¿’å·¥å…· (t-SNE, Isomap ç­‰)
from matplotlib.ticker import NullFormatter  # Matplotlib æ ¼å¼å·¥å…·
from scipy.spatial.distance import pdist  # è¨ˆç®—å‘é‡ä¹‹é–“è·é›¢
# import matplotlib  # Matplotlib ä¸»å¥—ä»¶
# import pickle  # Python å…§å»ºç‰©ä»¶åºåˆ—åŒ–å·¥å…·
# === è¨ˆç®—ç•°å¸¸ç†±åŠ›åœ– ===
def cal_anomaly_map(fs_list, ft_list, out_size=224, amap_mode='mul'):
    if amap_mode == 'mul':  # ä¹˜æ³•æ¨¡å¼
        anomaly_map = np.ones([out_size, out_size])  # åˆå§‹åŒ–ç‚ºå…¨ 1
    else:  # åŠ æ³•æ¨¡å¼
        anomaly_map = np.zeros([out_size, out_size])  # åˆå§‹åŒ–ç‚ºå…¨ 0
    a_map_list = []  # å„²å­˜æ¯ä¸€å±¤çš„ anomaly map
    for i in range(len(ft_list)):
        fs = fs_list[i]  # ç‰¹å¾µä¾†æº
        ft = ft_list[i]  # ç‰¹å¾µç›®æ¨™
        a_map = 1 - F.cosine_similarity(fs, ft)  # è¨ˆç®— cosine ç›¸ä¼¼åº¦ä¸¦è½‰æˆ anomaly åˆ†æ•¸
        a_map = torch.unsqueeze(a_map, dim=1)  # å¢åŠ ä¸€å€‹ç¶­åº¦ (batch, channel, h, w)
        a_map = F.interpolate(a_map,
                              size=out_size,
                              mode='bilinear',
                              align_corners=True)  # ä¸Šæ¡æ¨£è‡³è¼¸å‡ºå¤§å°
        a_map = a_map[0, 0, :, :].to('cpu').detach().numpy()  # è½‰ numpy
        a_map_list.append(a_map)  # ä¿å­˜ anomaly map
        if amap_mode == 'mul':  # ä¹˜æ³•èšåˆ
            anomaly_map *= a_map
        else:  # åŠ æ³•èšåˆ
            anomaly_map += a_map
    return anomaly_map, a_map_list  # å›å‚³ç¸½é«” anomaly map ä»¥åŠæ¯å±¤çš„ anomaly map
# === ç–ŠåŠ  anomaly map åˆ°åŸåœ– ===
def show_cam_on_image(img, anomaly_map):
    cam = np.float32(anomaly_map) / 255 + np.float32(img) / 255  # æ­£è¦åŒ–å¾ŒåŠ åœ¨åŸåœ–ä¸Š
    cam = cam / np.max(cam)  # ç¸®æ”¾åˆ° 0~1
    return np.uint8(255 * cam)  # å›å‚³ uint8 æ ¼å¼å½±åƒ
# === æœ€å°-æœ€å¤§æ­£è¦åŒ– ===
def min_max_norm(image):
    a_min, a_max = image.min(), image.max()
    return (image - a_min) / (a_max - a_min)
# === è½‰æ›æˆç†±åŠ›åœ– (colormap) ===
def cvt2heatmap(gray):
    heatmap = cv2.applyColorMap(np.uint8(gray),
                                cv2.COLORMAP_JET)  # OpenCV colormap
    return heatmap
# === è©•ä¼°å‡½å¼ ===
def evaluation(encoder, bn, decoder, dataloader, device, _class_=None):
    bn.eval()  # è¨­ç‚ºæ¨è«–æ¨¡å¼
    decoder.eval()
    gt_list_px = []  # pixel-level ground truth
    pr_list_px = []  # pixel-level prediction
    gt_list_sp = []  # image-level ground truth
    pr_list_sp = []  # image-level prediction
    aupro_list = []  # PRO è©•ä¼°
    with torch.no_grad():
        for img, gt, label, _ in dataloader:  # å¾ dataloader å–è³‡æ–™
            img = img.to(device)  # æŠŠåœ–ç‰‡é€åˆ° GPU/CPU
            inputs = encoder(img)  # å– encoder ç‰¹å¾µ
            outputs = decoder(bn(inputs))  # è§£ç¢¼å™¨è¼¸å‡º
            anomaly_map, _ = cal_anomaly_map(inputs,
                                             outputs,
                                             img.shape[-1],
                                             amap_mode='a')  # è¨ˆç®— anomaly map
            anomaly_map = gaussian_filter(anomaly_map, sigma=4)  # é«˜æ–¯æ¿¾æ³¢å¹³æ»‘
            # äºŒå€¼åŒ– ground truth
            gt[gt > 0.5] = 1
            gt[gt <= 0.5] = 0
            if label.item() != 0:  # å¦‚æœæ˜¯ç‘•ç–µé¡åˆ¥
                aupro_list.append(
                    compute_pro(
                        gt.squeeze(0).cpu().numpy().astype(int),
                        anomaly_map[np.newaxis, :, :]))
            # ç´¯ç©åƒç´ ç´š ground truth èˆ‡é æ¸¬
            gt_list_px.extend(gt.cpu().numpy().astype(int).ravel())
            pr_list_px.extend(anomaly_map.ravel())
            # ç´¯ç©åœ–ç‰‡ç´š (æ˜¯å¦æœ‰ç•°å¸¸)
            gt_list_sp.append(np.max(gt.cpu().numpy().astype(int)))
            pr_list_sp.append(np.max(anomaly_map))
        auroc_px = round(roc_auc_score(gt_list_px, pr_list_px), 3)  # è¨ˆç®—åƒç´ ç´š AUC
        auroc_sp = round(roc_auc_score(gt_list_sp, pr_list_sp), 3)  # è¨ˆç®—åœ–ç‰‡ç´š AUC
    return auroc_px, auroc_sp, round(np.mean(aupro_list), 3)
# === æ¸¬è©¦å‡½å¼ ===
def test(_class_):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'  # åˆ¤æ–·æ˜¯å¦ä½¿ç”¨ GPU
    print(device)
    print(_class_)
    data_transform, gt_transform = get_data_transforms(256, 256)  # è³‡æ–™å¢å¼·
    test_path = '../mvtec/' + _class_  # æ¸¬è©¦è³‡æ–™è·¯å¾‘
    ckp_path = './checkpoints/' + 'rm_1105_wres50_ff_mm_' + _class_ + '.pth'  # æ¨¡å‹ checkpoint è·¯å¾‘
    test_data = MVTecDataset(root=test_path,
                             transform=data_transform,
                             gt_transform=gt_transform,
                             phase="test")  # æ¸¬è©¦è³‡æ–™é›†
    test_dataloader = torch.utils.data.DataLoader(test_data,
                                                  batch_size=1,
                                                  shuffle=False)  # æ¸¬è©¦è³‡æ–™ loader
    encoder, bn = wide_resnet50_2(pretrained=True)  # å»ºç«‹ç·¨ç¢¼å™¨
    encoder = encoder.to(device)
    bn = bn.to(device)
    encoder.eval()
    decoder = de_wide_resnet50_2(pretrained=False)  # å»ºç«‹è§£ç¢¼å™¨
    decoder = decoder.to(device)
    ckp = torch.load(ckp_path)  # è¼‰å…¥ checkpoint
    for k, v in list(ckp['bn'].items()):  # ç§»é™¤ batchnorm çš„ memory æ¬„ä½
        if 'memory' in k:
            ckp['bn'].pop(k)
    decoder.load_state_dict(ckp['decoder'])
    bn.load_state_dict(ckp['bn'])
    auroc_px, auroc_sp, aupro_px = evaluation(encoder, bn, decoder,
                                              test_dataloader, device,
                                              _class_)  # åŸ·è¡Œè©•ä¼°
    print(_class_, ':', auroc_px, ',', auroc_sp, ',', aupro_px)
    return auroc_px
import os  # è¼‰å…¥ä½œæ¥­ç³»çµ±æ¨¡çµ„ï¼Œç”¨æ–¼æª”æ¡ˆè·¯å¾‘è™•ç†ã€å»ºç«‹è³‡æ–™å¤¾
# =============================
# å‡½å¼ï¼šå¯è¦–åŒ–æ¨¡å‹è¼¸å‡ºçµæœ
# =============================
def visualization(_arch_, _class_, save_path=None, ckp_path=None):
    print(f"ğŸ–¼ï¸ é–‹å§‹å¯è¦–åŒ–é¡åˆ¥: {_class_}")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'  # åˆ¤æ–·ä½¿ç”¨ GPU æˆ– CPU
    data_transform, gt_transform = get_data_transforms(256,
                                                       256)  # å–å¾—å½±åƒèˆ‡æ¨™è¨»çš„è³‡æ–™è½‰æ›æ–¹å¼
    test_path = f'./mvtec/{_class_}'  # æ¸¬è©¦è³‡æ–™é›†è·¯å¾‘ï¼Œèˆ‡ train() ä¸€è‡´
    # âœ… å¦‚æœæ²’æœ‰å¤–éƒ¨å‚³å…¥æ¬Šé‡æª”è·¯å¾‘ï¼Œå°±ä½¿ç”¨é è¨­å€¼
    if ckp_path is None:
        ckp_path = f'./checkpoints/{_arch_}_{_class_}.pth'
    # å»ºç«‹æ¸¬è©¦è³‡æ–™é›†
    test_data = MVTecDataset(root=test_path,
                             transform=data_transform,
                             gt_transform=gt_transform,
                             phase="test")
    # å»ºç«‹ DataLoader (ä¸€æ¬¡ä¸€å¼µåœ–ç‰‡ï¼Œä¸æ‰“äº‚é †åº)
    test_dataloader = torch.utils.data.DataLoader(test_data,
                                                  batch_size=1,
                                                  shuffle=False)
    # å»ºç«‹ç·¨ç¢¼å™¨èˆ‡ BatchNorm
    encoder, bn = wide_resnet50_2(
        pretrained=True)  # ä½¿ç”¨ Wide ResNet50_2 ä½œç‚º backbone
    encoder = encoder.to(device)
    bn = bn.to(device)
    encoder.eval()  # è¨­å®šç‚ºæ¨ç†æ¨¡å¼
    decoder = de_wide_resnet50_2(pretrained=False)  # è§£ç¢¼å™¨ (Decoder)
    decoder = decoder.to(device)
    
    # Fixed line - explicitly set weights_only=False to suppress warning
    ckp = torch.load(ckp_path, map_location=device, weights_only=False)
    
    for k in list(ckp['bn'].keys()):
        if 'memory' in k:  # ç§»é™¤ batch norm çš„ "memory" æ¬„ä½ï¼Œé¿å…è¼‰å…¥å¤±æ•—
            del ckp['bn'][k]
    decoder.load_state_dict(ckp['decoder'])
    bn.load_state_dict(ckp['bn'])
    # å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾
    save_dir = save_path if save_path else f'results/{_class_}'
    os.makedirs(save_dir, exist_ok=True)
    count = 0  # è¨ˆæ•¸å™¨ï¼šå·²è™•ç†åœ–ç‰‡æ•¸
    with torch.no_grad():  # é—œé–‰æ¢¯åº¦ï¼Œç¯€çœè¨˜æ†¶é«”
        for img, gt, label, _ in test_dataloader:  # é€å¼µè™•ç†æ¸¬è©¦è³‡æ–™
            if label.item() == 0:  # å¦‚æœæ˜¯æ­£å¸¸æ¨£æœ¬ï¼Œè·³é
                continue
            img = img.to(device)
            inputs = encoder(img)  # ç·¨ç¢¼å½±åƒç‰¹å¾µ
            outputs = decoder(bn(inputs))  # è§£ç¢¼é‡å»ºå½±åƒ
            # è¨ˆç®—ç•°å¸¸åœ– (Anomaly Map)
            anomaly_map, _ = cal_anomaly_map([inputs[-1]], [outputs[-1]],
                                             img.shape[-1],
                                             amap_mode='a')
            anomaly_map = gaussian_filter(anomaly_map, sigma=4)  # é«˜æ–¯å¹³æ»‘
            ano_map = min_max_norm(anomaly_map)  # æ­£è¦åŒ–åˆ° 0~1
            ano_map = cvt2heatmap(ano_map * 255)  # è½‰æ›æˆç†±åŠ›åœ–
            # å°‡åŸåœ–è½‰ç‚º numpy æ ¼å¼ï¼Œä¸¦æ¨™æº–åŒ–
            img_np = img.permute(0, 2, 3, 1).cpu().numpy()[0] * 255
            img_np = cv2.cvtColor(img_np.astype(np.uint8), cv2.COLOR_BGR2RGB)
            img_norm = np.uint8(min_max_norm(img_np) * 255)
            overlay = show_cam_on_image(img_norm, ano_map)  # ç–ŠåŠ ç†±åŠ›åœ–
            # å„²å­˜åŸåœ–èˆ‡ç–ŠåŠ ç†±åŠ›åœ–
            cv2.imwrite(f"{save_dir}/{count:03d}_org.png", img_norm)
            cv2.imwrite(f"{save_dir}/{count:03d}_ad.png", overlay)
            count += 1
    print(f"âœ… å¯è¦–åŒ–å®Œæˆï¼Œå…±å„²å­˜ {count} å¼µåœ–ç‰‡è‡³ {save_dir}")
import numpy as np
import pandas as pd
from numpy import ndarray
from skimage import measure
from sklearn.metrics import auc
from statistics import mean
# =============================
# å‡½å¼ï¼šè¨ˆç®— PRO æŒ‡æ¨™ (Pixel-wise Recall Overlap)
# =============================
def compute_pro(masks: ndarray, amaps: ndarray, num_th: int = 200) -> float:
    """è¨ˆç®—æ¯å€‹å€åŸŸé‡ç–Šç‡ï¼ˆPROï¼‰èˆ‡ FPR åœ¨ 0~0.3 å€é–“çš„ AUC"""
    # --- è³‡æ–™é©—è­‰ ---
    assert isinstance(amaps, ndarray), "amaps å¿…é ˆæ˜¯ ndarray"
    assert isinstance(masks, ndarray), "masks å¿…é ˆæ˜¯ ndarray"
    assert amaps.ndim == 3 and masks.ndim == 3, "amaps å’Œ masks å¿…é ˆæ˜¯ä¸‰ç¶­é™£åˆ—"
    assert amaps.shape == masks.shape, "amaps å’Œ masks çš„å½¢ç‹€å¿…é ˆä¸€è‡´"
    assert set(np.unique(masks)) <= {0, 1}, "masks å¿…é ˆæ˜¯äºŒå€¼ (0 æˆ– 1)"
    assert isinstance(num_th, int), "num_th å¿…é ˆæ˜¯æ•´æ•¸"
    # --- åˆå§‹åŒ– ---
    df = pd.DataFrame({
        "pro": pd.Series(dtype="float"),
        "fpr": pd.Series(dtype="float"),
        "threshold": pd.Series(dtype="float")
    })
    min_th, max_th = amaps.min(), amaps.max()
    thresholds = np.linspace(min_th, max_th, num_th)
    # --- é–¾å€¼æƒæ ---
    for th in thresholds:
        binary_amaps = (amaps > th).astype(np.uint8)  # é–¾å€¼äºŒå€¼åŒ–
        pros = []
        for binary_amap, mask in zip(binary_amaps, masks):
            labeled_mask = measure.label(mask)  # æ¨™è¨˜ mask å€åŸŸ
            for region in measure.regionprops(labeled_mask):
                coords = region.coords
                tp_pixels = binary_amap[coords[:, 0], coords[:, 1]].sum()
                pros.append(tp_pixels / region.area)  # å€åŸŸå…§ TP æ¯”ä¾‹
        inverse_masks = 1 - masks
        fp_pixels = np.logical_and(inverse_masks, binary_amaps).sum()
        fpr = fp_pixels / inverse_masks.sum()  # å½é™½ç‡

        new_row = pd.DataFrame([{
            "pro": mean(pros) if pros else 0,
            "fpr": fpr,
            "threshold": th
        }])
        # âœ… é¿å… concat ç©ºæˆ–å…¨ NA çš„ DataFrame
        if not new_row.isna().all(axis=None) and not new_row.empty:
            df = pd.concat([df, new_row], ignore_index=True)
    # --- FPR æ­£è¦åŒ–èˆ‡ AUC è¨ˆç®— ---
    df = df[df["fpr"] < 0.3]  # åªä¿ç•™ FPR < 0.3
    if df.empty or df["fpr"].max() == 0:
        return 0.0
    df["fpr"] = df["fpr"] / df["fpr"].max()  # FPR æ­£è¦åŒ–
    pro_auc = auc(df["fpr"], df["pro"])  # è¨ˆç®— AUC
    return pro_auc